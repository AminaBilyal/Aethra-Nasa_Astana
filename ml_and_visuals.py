# -*- coding: utf-8 -*-
"""ML and visuals.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fFgYsQ4HcgTtmyWuFXUFthCFgIbtX_e2
"""

#Installing libraries
!pip install pandas matplotlib seaborn -q

#Import libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from datetime import datetime

#Creating dataset csv. files for distrcts of Astana
def create_astana_data():
    districts = {
        'Almaty': {'lat': 51.15, 'lon': 71.45, 'population': 350000},
        'Saryarka': {'lat': 51.20, 'lon': 71.40, 'population': 280000},
        'Yesil': {'lat': 51.18, 'lon': 71.35, 'population': 320000},
        'Nura': {'lat': 51.12, 'lon': 71.50, 'population': 190000},
        'Baikonur': {'lat': 51.22, 'lon': 71.48, 'population': 150000}
    }

    all_data = []
    dates = pd.date_range('2023-01-01', '2023-12-31', freq='D')

    for district, info in districts.items():
        for i, date in enumerate(dates):
            base_temp = 5 + 25 * np.sin(2 * np.pi * i / 365)

            if district == 'Almaty':
                temp = base_temp + np.random.normal(0, 2)
                pm25 = np.random.normal(22, 5)
            elif district == 'Saryarka':
                temp = base_temp + np.random.normal(1, 2)
                pm25 = np.random.normal(25, 6)
            elif district == 'Yesil':
                temp = base_temp + np.random.normal(-1, 1.5)
                pm25 = np.random.normal(15, 4)
            elif district == 'Nura':
                temp = base_temp + np.random.normal(0, 2)
                pm25 = np.random.normal(18, 5)
            else:
                temp = base_temp + np.random.normal(0, 2)
                pm25 = np.random.normal(20, 5)

            humidity = 60 + 20 * np.sin(2 * np.pi * i / 365 + np.pi/2) + np.random.normal(0, 5)

            all_data.append({
                'date': date,
                'district': district,
                'temperature': round(temp, 1),
                'humidity': max(30, min(95, round(humidity, 1))),
                'pm25': max(5, round(pm25, 1)),
                'population': info['population'],
                'latitude': info['lat'],
                'longitude': info['lon']
            })

    return pd.DataFrame(all_data)

#DataFrame
df = create_astana_data()

#CSV
df.to_csv('astana_districts_data.csv', index=False, encoding='utf-8')

#Output
print("ПFirst 5 lines of data:")
print(df.head())
print("\n PM2.5 statistics :")
print(df.groupby('district')['pm25'].describe())

#Graphic
fig, axes = plt.subplots(2, 2, figsize=(15, 12))

#PM2.5
pm25_means = df.groupby('district')['pm25'].mean().sort_values(ascending=False)
axes[0,0].bar(pm25_means.index, pm25_means.values)
axes[0,0].set_title('Average level of PM2.5 in districts')
axes[0,0].set_ylabel('PM2.5')
axes[0,0].tick_params(axis='x', rotation=45)

#Monthly temperature
df['month'] = df['date'].dt.month
monthly_temp = df.groupby(['month', 'district'])['temperature'].mean().reset_index()
for district in df['district'].unique():
    district_data = monthly_temp[monthly_temp['district'] == district]
    axes[0,1].plot(district_data['month'], district_data['temperature'], marker='o', label=district)
axes[0,1].set_title('Monthly temperature')
axes[0,1].set_xlabel('Month')
axes[0,1].set_ylabel('Tempreature')
axes[0,1].legend()
axes[0,1].set_xticks(range(1, 13))
#Population vs pollution
population_pm25 = df.groupby('district').agg({'population': 'first', 'pm25': 'mean'}).reset_index()
axes[1,0].scatter(population_pm25['population'], population_pm25['pm25'], s=100)
for i, row in population_pm25.iterrows():
    axes[1,0].annotate(row['district'], (row['population'], row['pm25']), xytext=(5, 5), textcoords='offset points')
axes[1,0].set_title('Population vs PM2.5')
axes[1,0].set_xlabel('Population')
axes[1,0].set_ylabel('PM2.5')

#Humidity
sns.boxplot(data=df, x='district', y='humidity', ax=axes[1,1])
axes[1,1].set_title('District humidity')
axes[1,1].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()

#Analysis
worst_district = df.groupby('district')['pm25'].mean().idxmax()


best_district = df.groupby('district')['pm25'].mean().idxmin()

print(f"The most polluted district: {worst_district}")
print(f"The most safe ands clean district: {best_district}")

#Downloading the file
from google.colab import files
files.download('astana_districts_data.csv')
print("The file downloaded")

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error
import joblib

# Create dataset for Astana districts
def create_astana_data():
    districts = {
        'Almaty': {'lat': 51.15, 'lon': 71.45, 'population': 350000, 'green_spaces': 12},
        'Saryarka': {'lat': 51.20, 'lon': 71.40, 'population': 280000, 'green_spaces': 8},
        'Yesil': {'lat': 51.18, 'lon': 71.35, 'population': 320000, 'green_spaces': 15},
        'Nura': {'lat': 51.12, 'lon': 71.50, 'population': 190000, 'green_spaces': 10},
        'Baikonur': {'lat': 51.22, 'lon': 71.48, 'population': 150000, 'green_spaces': 9}
    }

    all_data = []
    dates = pd.date_range('2023-01-01', '2023-12-31', freq='D')

    for district, info in districts.items():
        for i, date in enumerate(dates):
            base_temp = 5 + 25 * np.sin(2 * np.pi * i / 365)

            # District-specific variations
            if district == 'Saryarka':
                temp = base_temp + np.random.normal(2, 1.5)
                pm25 = np.random.normal(28, 6)
                traffic = np.random.normal(80000, 10000)
            elif district == 'Almaty':
                temp = base_temp + np.random.normal(1, 2)
                pm25 = np.random.normal(22, 5)
                traffic = np.random.normal(95000, 15000)
            elif district == 'Yesil':
                temp = base_temp + np.random.normal(-1, 1)
                pm25 = np.random.normal(15, 3)
                traffic = np.random.normal(60000, 8000)
            else:
                temp = base_temp + np.random.normal(0, 1.5)
                pm25 = np.random.normal(20, 4)
                traffic = np.random.normal(50000, 7000)

            humidity = 60 + 20 * np.sin(2 * np.pi * i / 365 + np.pi/2) + np.random.normal(0, 5)
            park_need = max(0, (pm25 - 10) * info['population'] / 10000)

            all_data.append({
                'date': date,
                'district': district,
                'temperature': round(temp, 1),
                'humidity': max(30, min(95, round(humidity, 1))),
                'pm25': max(5, round(pm25, 1)),
                'population': info['population'],
                'green_spaces': info['green_spaces'],
                'traffic_volume': max(10000, traffic),
                'park_need_index': round(park_need, 1),
                'latitude': info['lat'],
                'longitude': info['lon']
            })

    return pd.DataFrame(all_data)

# Create and analyze data
df = create_astana_data()

print("ASTANA URBAN PLANNING ANALYSIS")
print("=" * 50)

# Pollution analysis
pm25_ranking = df.groupby('district')['pm25'].mean().sort_values(ascending=False)
print("\nDISTRICT POLLUTION RANKING:")
for district, value in pm25_ranking.items():
    print(f"{district}: {value:.1f} µg/m³")

# Park needs analysis
park_priority = df.groupby('district')['park_need_index'].mean().sort_values(ascending=False)
print("\nPARK DEVELOPMENT PRIORITIES:")
for district, score in park_priority.items():
    print(f"{district}: {score:.0f} points")

# Prepare data for ML
df_ml = df.copy()
df_ml['day_of_year'] = df_ml['date'].dt.dayofyear
df_ml['month'] = df_ml['date'].dt.month

features = ['temperature', 'humidity', 'population', 'green_spaces', 'traffic_volume', 'month']
X = df_ml[features]
y = df_ml['pm25']

# Train model
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = RandomForestRegressor(n_estimators=50, max_depth=10, n_jobs=-1, random_state=42)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)

print(f"\nMODEL ACCURACY: {mae:.2f} µg/m³")

# Create visualizations
fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# Plot 1: Pollution by district
pm25_data = df.groupby('district')['pm25'].mean().sort_values(ascending=False)
axes[0,0].bar(pm25_data.index, pm25_data.values)
axes[0,0].set_title('Average PM2.5 by District')
axes[0,0].axhline(y=25, color='red', linestyle='--', label='Unhealthy level')
axes[0,0].axhline(y=15, color='orange', linestyle='--', label='Moderate level')
axes[0,0].legend()

# Plot 2: Green spaces vs pollution
green_pm25 = df.groupby('district').agg({'green_spaces': 'first', 'pm25': 'mean'})
axes[0,1].scatter(green_pm25['green_spaces'], green_pm25['pm25'], s=100)
for i, row in green_pm25.iterrows():
    axes[0,1].annotate(i, (row['green_spaces'], row['pm25']), xytext=(5,5), textcoords='offset points')
axes[0,1].set_xlabel('Green Spaces (km²)')
axes[0,1].set_ylabel('PM2.5')
axes[0,1].set_title('Green Spaces vs Air Pollution')

# Add 'month' column to df for plotting
df['month'] = df['date'].dt.month

# Plot 3: Seasonal patterns
monthly_pm25 = df.groupby('month')['pm25'].mean()
axes[1,0].plot(monthly_pm25.index, monthly_pm25.values, marker='o', linewidth=2)
axes[1,0].set_xlabel('Month')
axes[1,0].set_ylabel('PM2.5')
axes[1,0].set_title('Seasonal Pollution Patterns')
axes[1,0].set_xticks(range(1,13))

# Plot 4: Feature importance
feature_importance = pd.DataFrame({
    'feature': features,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=True)
axes[1,1].barh(feature_importance['feature'], feature_importance['importance'])
axes[1,1].set_title('Feature Importance for PM2.5 Prediction')

plt.tight_layout()
plt.show()

# Recommendations
print("\nURBAN PLANNING RECOMMENDATIONS:")
print("=" * 40)

worst_district = pm25_ranking.index[0]
best_district = pm25_ranking.index[-1]

print(f"\nPriority District: {worst_district}")
print("Actions: air quality monitoring, green infrastructure, traffic management")

print(f"\nBest Practice District: {best_district}")
print("Study and replicate successful strategies")

print(f"\nImmediate Park Development: {park_priority.index[0]}, {park_priority.index[1]}")

high_risk_days = df[df['pm25'] > 25].groupby('district').size()
print(f"\nHigh Pollution Days per Year:")
for district, days in high_risk_days.items():
    print(f"{district}: {days} days")

# Save results
df.to_csv('astana_analysis.csv', index=False)
joblib.dump(model, 'pollution_model.pkl')

print("\nFiles saved: astana_analysis.csv, pollution_model.pkl")

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# import joblib
# import matplotlib.pyplot as plt
# import seaborn as sns
# import numpy as np
# 
# st.title('Astana Urban Planning Analysis')
# 
# # Load data and model
# try:
#     df = pd.read_csv('astana_analysis.csv')
#     model = joblib.load('pollution_model.pkl')
# except FileNotFoundError:
#     st.error("Data files not found. Please run the previous cells to generate the data and model.")
#     st.stop()
# 
# st.header('Pollution and Urban Planning Insights')
# 
# # Display pollution ranking
# st.subheader('District Pollution Ranking (Average PM2.5)')
# pm25_ranking = df.groupby('district')['pm25'].mean().sort_values(ascending=False)
# st.bar_chart(pm25_ranking)
# st.write("Higher PM2.5 indicates higher pollution levels.")
# 
# # Display park development priorities
# st.subheader('Park Development Priorities (Park Need Index)')
# park_priority = df.groupby('district')['park_need_index'].mean().sort_values(ascending=False)
# st.bar_chart(park_priority)
# st.write("Higher index indicates a greater need for green spaces based on pollution and population.")
# 
# # Display high pollution days
# st.subheader('High Pollution Days per Year (> 25 µg/m³)')
# high_risk_days = df[df['pm25'] > 25].groupby('district').size().sort_values(ascending=False)
# st.bar_chart(high_risk_days)
# st.write("Number of days each district exceeded the unhealthy PM2.5 level.")
# 
# # Seasonal patterns
# st.subheader('Seasonal Pollution Patterns (Average Monthly PM2.5)')
# df['month'] = pd.to_datetime(df['date']).dt.month
# monthly_pm25 = df.groupby('month')['pm25'].mean()
# st.line_chart(monthly_pm25)
# st.write("Average PM2.5 levels throughout the year.")
# 
# # Feature importance for PM2.5 prediction
# st.subheader('Feature Importance for PM2.5 Prediction')
# # Assuming 'model' is a trained RandomForestRegressor and 'features' list is available from the training code
# features = ['temperature', 'humidity', 'population', 'green_spaces', 'traffic_volume', 'month'] # Define features again for the app
# feature_importance = pd.DataFrame({
#     'feature': features,
#     'importance': model.feature_importances_
# }).sort_values('importance', ascending=False)
# st.bar_chart(feature_importance.set_index('feature'))
# st.write("Relative importance of different factors in predicting PM2.5 levels.")
# 
# st.header('Recommendations')
# worst_district = pm25_ranking.index[0]
# best_district = pm25_ranking.index[-1]
# st.write(f"**Priority District for Intervention:** {worst_district}")
# st.write("Focus on air quality monitoring, increasing green infrastructure, and implementing traffic management solutions in this area.")
# st.write(f"**Best Practice District:** {best_district}")
# st.write("Study and potentially replicate successful urban planning and environmental strategies from this district.")
# st.write(f"**Immediate Park Development Needs:** {park_priority.index[0]} and {park_priority.index[1]}")
# st.write("These districts show the highest combined need for green spaces.")
# 
# st.markdown("---")
# st.write("This analysis is based on simulated data for Astana districts.")
#

!pip install streamlit -q
!pip install pyngrok -q
#saving data to CSV
df.to_csv("astana_analysis.csv", index = False)
print("'astana_analysis.csv' succesfully saved")
#saving ML-model
import joblib
joblib.dump(model, 'pollution_model.pkl')
print(" 'pollution_model.pkl' succesfully saved")
#checking the files
import os
print("Files:")
for file in ['astana_analysis.csv','pollution_model.pkl' ]:
  if os.path.exists(file):
    print(f"yes  {file}")
  else:
    print(f"no  {file}")
    from google.colab import files
files.download('pollution_model.pkl')

!pip install streamlit streamlit-folium pandas matplotlib folium pyngrok
from pyngrok import ngrok
from google.colab import userdata

# Get ngrok authtoken from Colab secrets
NGROK_AUTH_TOKEN = userdata.get('NGROK_AUTH_TOKEN')
ngrok.set_auth_token(NGROK_AUTH_TOKEN)

#Streamlit
!streamlit run app.py &

# Открываем туннель
url = ngrok.connect(8501)
print(" Открой сайт по ссылке ниже:")
print(url)